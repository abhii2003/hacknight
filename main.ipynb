{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91936\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\components\\chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 09:36:26,079 - INFO - HTTP Request: GET http://127.0.0.1:7870/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-03-07 09:36:26,092 - INFO - HTTP Request: HEAD http://127.0.0.1:7870/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 09:36:48,528 - INFO - Found 2 images\n",
      "2025-03-07 09:36:55,903 - INFO - Sending image for captioning\n",
      "2025-03-07 09:36:55,915 - INFO - Sending image for captioning\n",
      "2025-03-07 09:38:46,580 - INFO - Successfully generated caption\n",
      "2025-03-07 09:40:24,275 - INFO - Successfully generated caption\n",
      "2025-03-07 09:40:24,276 - INFO - Generated 2 captions\n",
      "2025-03-07 09:40:25,857 - INFO - Extracted 25 characters of text\n",
      "2025-03-07 09:40:25,858 - INFO - Generating summary\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "import requests\n",
    "import io\n",
    "import base64\n",
    "import re\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gradio as gr\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "API_URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "# Check if URL contains adult content\n",
    "def is_adult_content(url):\n",
    "    adult_keywords = ['porn', 'xxx', 'adult', 'sex', 'nude', 'nsfw', 'explicit', 'pornography']\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    domain = parsed_url.netloc.lower()\n",
    "    path = parsed_url.path.lower()\n",
    "    \n",
    "    for keyword in adult_keywords:\n",
    "        if keyword in domain or keyword in path:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Extract images from a webpage with improved handling\n",
    "def extract_images_from_url(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        images = []\n",
    "        seen_urls = set()  # To avoid duplicates\n",
    "        \n",
    "        # Find all img tags\n",
    "        for img in soup.find_all('img'):\n",
    "            img_url = None\n",
    "            # Check various image attributes in priority order\n",
    "            for attr in ['src', 'data-src', 'data-lazy-src', 'data-original', 'data-srcset', 'srcset']:\n",
    "                if attr in img.attrs and img[attr]:\n",
    "                    img_url = img[attr]\n",
    "                    # Handle srcset by taking the first URL\n",
    "                    if attr in ['srcset', 'data-srcset'] and ' ' in img[attr]:\n",
    "                        img_url = img[attr].split(' ')[0]\n",
    "                    break\n",
    "            \n",
    "            if img_url:\n",
    "                # Skip base64 encoded images\n",
    "                if img_url.startswith('data:image'):\n",
    "                    continue\n",
    "                    \n",
    "                # Convert relative URLs to absolute\n",
    "                if img_url.startswith('/'):\n",
    "                    parsed_url = urllib.parse.urlparse(url)\n",
    "                    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "                    img_url = urllib.parse.urljoin(base_url, img_url)\n",
    "                elif not img_url.startswith(('http://', 'https://')):\n",
    "                    img_url = urllib.parse.urljoin(url, img_url)\n",
    "                \n",
    "                # Skip duplicate images\n",
    "                if img_url in seen_urls:\n",
    "                    continue\n",
    "                seen_urls.add(img_url)\n",
    "                \n",
    "                # Filter out common icons and tracking pixels based on URL patterns\n",
    "                if any(pattern in img_url.lower() for pattern in ['icon', 'logo', 'pixel', 'tracking', 'avatar', 'blank.gif']):\n",
    "                    continue\n",
    "                \n",
    "                # Filter out small icons, spacers, and tracking pixels\n",
    "                if 'width' in img.attrs and 'height' in img.attrs:\n",
    "                    try:\n",
    "                        width = int(img['width'])\n",
    "                        height = int(img['height'])\n",
    "                        if width < 100 or height < 100:\n",
    "                            continue\n",
    "                    except (ValueError, TypeError):\n",
    "                        pass\n",
    "                \n",
    "                images.append(img_url)\n",
    "        \n",
    "        # Return all images - no arbitrary limit\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting images: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Extract meaningful text from a webpage\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove navigation, footer, sidebar, and other non-content elements\n",
    "        for tag in soup.find_all(['nav', 'footer', 'aside']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        for tag in soup.find_all(class_=lambda x: x and any(c in str(x).lower() for c in ['nav', 'menu', 'footer', 'sidebar', 'comment', 'header', 'banner', 'ad-', 'advertisement'])):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Look for main content containers\n",
    "        main_content = soup.find(['main', 'article', 'section', 'div'], class_=lambda x: x and any(c in str(x).lower() for c in ['content', 'article', 'post', 'entry', 'main']))\n",
    "        \n",
    "        if main_content:\n",
    "            elements = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'])\n",
    "        else:\n",
    "            # Fallback to all content\n",
    "            elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'])\n",
    "        \n",
    "        # Filter out empty paragraphs and very short text\n",
    "        texts = []\n",
    "        for el in elements:\n",
    "            text = el.get_text(strip=True)\n",
    "            if len(text) > 20:  # Filter out very short text fragments\n",
    "                texts.append(text)\n",
    "        \n",
    "        if not texts:\n",
    "            return \"No meaningful text found.\"\n",
    "        \n",
    "        return \" \".join(texts)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text: {str(e)}\")\n",
    "        return f\"Failed to extract text: {str(e)}\"\n",
    "\n",
    "# Encode an image with robust error handling\n",
    "def encode_image(image_url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(image_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check content type to verify it's an image\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if not content_type.startswith('image/'):\n",
    "            logger.warning(f\"Not an image: {image_url} (Content-Type: {content_type})\")\n",
    "            return None\n",
    "        \n",
    "        # Save content to BytesIO\n",
    "        img_content = response.content\n",
    "        \n",
    "        # Verify image can be opened with PIL\n",
    "        try:\n",
    "            with Image.open(io.BytesIO(img_content)) as img:\n",
    "                img.verify()  # Verify it's a valid image\n",
    "            return img_content\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Invalid image format: {image_url} - {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encoding image {image_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Generate image captions with proper resource management - FIXED FUNCTION\n",
    "def generate_caption(image_data):\n",
    "    if not image_data:\n",
    "        return \"Failed to process image.\"\n",
    "\n",
    "    try:\n",
    "        # Create a new BytesIO object for each operation to avoid closing issues\n",
    "        img_bytes = io.BytesIO(image_data)\n",
    "        \n",
    "        # Open and validate the image\n",
    "        with Image.open(img_bytes) as image:\n",
    "            img_format = image.format\n",
    "            if not img_format:\n",
    "                logger.warning(\"Image has no format information\")\n",
    "                img_format = \"JPEG\"  # Default to JPEG\n",
    "            \n",
    "            # Get image dimensions for potential resizing\n",
    "            width, height = image.size\n",
    "            \n",
    "            # Create a new copy for processing to avoid resource issues\n",
    "            image_copy = image.copy()\n",
    "        \n",
    "        # Resize large images to save bandwidth\n",
    "        max_size = 1024\n",
    "        if max(width, height) > max_size:\n",
    "            if width > height:\n",
    "                new_width = max_size\n",
    "                new_height = int(height * (max_size / width))\n",
    "            else:\n",
    "                new_height = max_size\n",
    "                new_width = int(width * (max_size / height))\n",
    "            \n",
    "            # Use the copied image for resizing\n",
    "            image_copy = image_copy.resize((new_width, new_height), Image.LANCZOS)\n",
    "        \n",
    "        # Handle image mode conversion in a separate buffer\n",
    "        buffered = io.BytesIO()\n",
    "        \n",
    "        if image_copy.mode in ['RGBA', 'LA']:\n",
    "            # Convert to RGB\n",
    "            background = Image.new('RGB', image_copy.size, (255, 255, 255))\n",
    "            if image_copy.mode == 'RGBA':\n",
    "                background.paste(image_copy, mask=image_copy.split()[3])\n",
    "            else:\n",
    "                background.paste(image_copy, mask=image_copy.split()[1])\n",
    "            background.save(buffered, format=\"JPEG\", quality=85)\n",
    "            # Explicitly close the background image\n",
    "            background.close()\n",
    "        else:\n",
    "            # Make sure we're in a common format (RGB or L)\n",
    "            if image_copy.mode not in ['RGB', 'L']:\n",
    "                image_copy = image_copy.convert('RGB')\n",
    "            image_copy.save(buffered, format=\"JPEG\", quality=85)\n",
    "        \n",
    "        # Get base64 encoded data and ensure resources are closed\n",
    "        buffered.seek(0)\n",
    "        base64_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "        \n",
    "        # Explicitly close resources\n",
    "        buffered.close()\n",
    "        image_copy.close()\n",
    "        \n",
    "        # Validate base64 data\n",
    "        if not base64_image:\n",
    "            logger.error(\"Failed to encode image to base64\")\n",
    "            return \"Error generating caption: Failed to encode image\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"llava\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail. What is shown? What is the main subject? What key elements are visible? Provide a thorough but concise caption.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "            ]}],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 30000000\n",
    "        }\n",
    "\n",
    "        logger.info(\"Sending image for captioning\")\n",
    "        response = requests.post(API_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        caption = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        logger.info(\"Successfully generated caption\")\n",
    "        return caption\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating caption: {str(e)}\")\n",
    "        return f\"Image description unavailable\"\n",
    "\n",
    "# Process images with improved resource management\n",
    "def process_images(images, max_workers=3):\n",
    "    captions = []\n",
    "    \n",
    "    if not images:\n",
    "        return [], []\n",
    "    \n",
    "    # First encode all images\n",
    "    encoded_images = []\n",
    "    for img_url in images:\n",
    "        encoded = encode_image(img_url)\n",
    "        if encoded:\n",
    "            encoded_images.append((img_url, encoded))\n",
    "    \n",
    "    if not encoded_images:\n",
    "        return [], []\n",
    "    \n",
    "    # Then caption them with better error handling\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_img = {executor.submit(generate_caption, img_data): img_url \n",
    "                         for img_url, img_data in encoded_images}\n",
    "        \n",
    "        completed_captions = []\n",
    "        for future in as_completed(future_to_img):\n",
    "            img_url = future_to_img[future]\n",
    "                \n",
    "            try:\n",
    "                caption = future.result()\n",
    "                if caption and not caption.startswith(\"Error\"):\n",
    "                    completed_captions.append((img_url, caption))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Exception while captioning image {img_url}: {str(e)}\")\n",
    "    \n",
    "    # Return results sorted by original image order\n",
    "    result_urls = []\n",
    "    result_captions = []\n",
    "    for url, data in encoded_images:\n",
    "        for img_url, caption in completed_captions:\n",
    "            if url == img_url:\n",
    "                result_urls.append(img_url)\n",
    "                result_captions.append(caption)\n",
    "                break\n",
    "    \n",
    "    return result_urls, result_captions\n",
    "\n",
    "# Process a URL\n",
    "def process_url(url):\n",
    "    if is_adult_content(url):\n",
    "        return [], [], \"Sorry, I cannot process adult content.\"\n",
    "    \n",
    "    try:\n",
    "        # Extract images\n",
    "        images = extract_images_from_url(url)\n",
    "        logger.info(f\"Found {len(images)} images\")\n",
    "        \n",
    "        # Process images concurrently\n",
    "        image_urls, captions = process_images(images)\n",
    "        logger.info(f\"Generated {len(captions)} captions\")\n",
    "        \n",
    "        # Extract text from webpage\n",
    "        text = extract_text_from_url(url)\n",
    "        logger.info(f\"Extracted {len(text)} characters of text\")\n",
    "        \n",
    "        # If text extraction failed but we have images, still continue\n",
    "        if text.startswith(\"Failed to extract text\") and captions:\n",
    "            text = \"Text extraction failed, but image content was analyzed.\"\n",
    "        \n",
    "        # Combine captions with text for summarization\n",
    "        combined_text = text\n",
    "        if captions:\n",
    "            caption_text = \"\\n\\nImage descriptions:\\n\" + \"\\n\".join([f\"- {caption}\" for caption in captions])\n",
    "            combined_text = combined_text + caption_text\n",
    "        \n",
    "        # Generate summary\n",
    "        logger.info(\"Generating summary\")\n",
    "        summary = summarize_text(combined_text)\n",
    "        \n",
    "        return image_urls, captions, summary\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_url: {str(e)}\")\n",
    "        return [], [], f\"Error processing URL: {str(e)}\"\n",
    "\n",
    "# Summarize extracted text with image captions included\n",
    "def summarize_text(text):\n",
    "    if not text or text == \"No meaningful text found.\" or text.startswith(\"Failed to extract text\"):\n",
    "        return \"No content found to summarize.\"\n",
    "\n",
    "    # Increase text length limit for more content\n",
    "    text = text[:8000] if len(text) > 8000 else text\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"slim-summary-phi-3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert content analyst and summarizer. Create a comprehensive summary that includes key insights from both the text and image descriptions. Organize the summary in a structured format with main points and supporting details.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Please summarize the following webpage content, including both text and image descriptions:\\n\\n{text}\"}\n",
    "        ],\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 1000000\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(API_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Summarization failed: {str(e)}\")\n",
    "        return \"I was able to extract content but couldn't generate a summary. Here are the key points from the images and text I found.\"\n",
    "\n",
    "# Extract URL from text with improved validation\n",
    "def extract_url(text):\n",
    "    url_pattern = re.compile(r'(https?://\\S+|www\\.\\S+)')\n",
    "    match = url_pattern.search(text)\n",
    "    \n",
    "    if not match:\n",
    "        return None\n",
    "        \n",
    "    url = match.group(0)\n",
    "    \n",
    "    # Handle URLs that don't start with http/https\n",
    "    if url.startswith('www.'):\n",
    "        url = 'https://' + url\n",
    "        \n",
    "    # Remove trailing punctuation that might have been captured\n",
    "    url = url.rstrip('.,;:!?)')\n",
    "    \n",
    "    # Validate URL format\n",
    "    try:\n",
    "        result = urllib.parse.urlparse(url)\n",
    "        return url if all([result.scheme, result.netloc]) else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Chatbot with improved error handling\n",
    "def chatbot(message, history):\n",
    "    if not message.strip():\n",
    "        return history, \"\"\n",
    "        \n",
    "    url = extract_url(message)\n",
    "    response = \"\"\n",
    "\n",
    "    if url:\n",
    "        # Show processing message\n",
    "        processing_msg = f\"Processing {url}... This may take some time depending on the content amount. Please wait for complete results.\"\n",
    "        history.append((message, processing_msg))\n",
    "        yield history, \"\"\n",
    "        \n",
    "        # Check for adult content\n",
    "        if is_adult_content(url):\n",
    "            response = \"Sorry, I cannot process adult content.\"\n",
    "            history[-1] = (message, response)\n",
    "            yield history, \"\"\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Process URL\n",
    "            images, captions, summary = process_url(url)\n",
    "            \n",
    "            if not images and not summary:\n",
    "                response = f\"I was unable to extract meaningful content from {url}. The site may block scraping or use complex JavaScript.\"\n",
    "            elif summary.startswith(\"Error\") or summary.startswith(\"Summarization failed\"):\n",
    "                # Fallback to just showing what we have\n",
    "                response = f\"**Partial results from {url}:**\\n\\n\"\n",
    "                if images and captions:\n",
    "                    response += \"**Image Descriptions:**\\n\\n\"\n",
    "                    for i, caption in enumerate(captions):\n",
    "                        response += f\"**Image {i+1}:** {caption}\\n\\n\"\n",
    "                else:\n",
    "                    response += \"I was able to find the page but couldn't fully process its content. Try a different URL or ask a simpler question.\"\n",
    "            else:\n",
    "                response = f\"**Summary of {url}:**\\n\\n{summary}\\n\\n\"\n",
    "                if captions:\n",
    "                    response += \"**Image Descriptions:**\\n\\n\"\n",
    "                    for i, caption in enumerate(captions):\n",
    "                        response += f\"**Image {i+1}:** {caption}\\n\\n\"\n",
    "            \n",
    "            history[-1] = (message, response)\n",
    "            yield history, \"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in chatbot URL processing: {str(e)}\")\n",
    "            response = f\"I encountered an error when processing {url}. The site may be unavailable or not compatible with my analysis capabilities.\"\n",
    "            history[-1] = (message, response)\n",
    "            yield history, \"\"\n",
    "    \n",
    "    elif history:\n",
    "        # Context-aware Q&A after summarization\n",
    "        try:\n",
    "            # Get all previous content for context\n",
    "            context = \"\\n\\n\".join([turn[1] for turn in history if \"Summary of\" in turn[1] or \"Image Descriptions:\" in turn[1]])\n",
    "            \n",
    "            if not context:\n",
    "                response = \"Please provide a URL first so I can analyze content.\"\n",
    "            else:\n",
    "                payload = {\n",
    "                    \"model\": \"slim-summary-phi-3\",\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"Answer based on the previously summarized content. Be specific and cite information from the summary. If the question cannot be answered based on the available information, clearly state that.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"Based on this summarized content, answer the following question: {message}\\n\\nSummarized content:\\n{context}\"}\n",
    "                    ],\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"max_tokens\": 500000\n",
    "                }\n",
    "\n",
    "                api_response = requests.post(API_URL, json=payload)\n",
    "                api_response.raise_for_status()\n",
    "                response = api_response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Q&A: {str(e)}\")\n",
    "            response = \"I couldn't process your question about the previous content. Could you try rephrasing it?\"\n",
    "    else:\n",
    "        response = \"I'm here to summarize web content. Please provide a URL to begin.\"\n",
    "\n",
    "    history.append((message, response))\n",
    "    yield history, \"\"\n",
    "\n",
    "# Gradio UI with better styling and functionality\n",
    "with gr.Blocks(css=\"footer {visibility: hidden}\") as iface:\n",
    "    gr.Markdown(\"# Enhanced Web Content Analyzer\")\n",
    "    gr.Markdown(\"\"\"\n",
    "    Enter a URL to extract and analyze web content:\n",
    "    1. Captions all meaningful images without timeouts\n",
    "    2. Extracts relevant text (ignores navigation, ads, etc.)\n",
    "    3. Generates a comprehensive summary using all available content\n",
    "    \n",
    "    \n",
    "    Note: Processing may take longer for content-heavy sites, but will be more complete.\n",
    "    \"\"\")\n",
    "    \n",
    "    chatbot_interface = gr.Chatbot(label=\"Chat\", height=600, bubble_full_width=False)\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(placeholder=\"Enter a URL ...\", show_label=False)\n",
    "        submit = gr.Button(\"Send\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    msg.submit(chatbot, inputs=[msg, chatbot_interface], outputs=[chatbot_interface, msg])\n",
    "    submit.click(chatbot, inputs=[msg, chatbot_interface], outputs=[chatbot_interface, msg])\n",
    "\n",
    "# Launch\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        iface.launch()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error launching interface: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
